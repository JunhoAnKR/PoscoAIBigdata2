{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 받기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\", one_hot=True)\n",
    "train_data = mnist.train.images\n",
    "train_label = mnist.train.labels\n",
    "test_data = mnist.test.images\n",
    "test_label = mnist.test.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 데이터 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000, 784)\n",
      "(55000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# size of MNIST\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(test_data.shape)\n",
    "print(test_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing (데이터 전처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (1)\n",
    "\n",
    "\n",
    "## Loss function (손실 함수) : Cross Entropy\n",
    "\n",
    "# <center> \\\\( L(y_i, f(x_i; W)) = -\\frac{1}{n}\\sum_{i=1}^{n}\\sum_{k=1} y_{i,j} log(f(x_i)_k)\\\\)</center>\n",
    "\n",
    "\n",
    "#### get_cross_entropy_loss 함수의 내용을 완성하세요.\n",
    "#### (Hint : (1) tf.reduce_mean(), tf.reduce_sum(), tf.log() (2) Tensor dimension에 유의 (3) log 함수 사용 시 epsilon 사용하세요.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cross_entropy_loss(y_true, y_hat, epsilon=1e-8):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        ###################################################################\n",
    "        #                    Implementation 1                             #\n",
    "        ###################################################################\n",
    "        loss = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(y_hat), axis = 1))\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_accuracy(y_true, y_hat):\n",
    "    \"\"\"\n",
    "    compute cross entropy\n",
    "\n",
    "    Args:\n",
    "        y_true: true label\n",
    "        y_hat: predicted label\n",
    "        epsilon: small value to prevent NaN in log\n",
    "\n",
    "    Returns:\n",
    "        cross entropy loss\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy'):\n",
    "        # Compare the highest indices between the predicted label and the true label\n",
    "        correct_prediction = tf.equal(tf.argmax(y_hat, 1), tf.argmax(y_true, 1), name='correct_prediction')\n",
    "        # Compute accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameter (하이퍼 파라미터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hypyerparameters\n",
    "learning_rate = 0.001\n",
    "max_iter = 2000\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (2)\n",
    "## Linear Classifier (선형 분류기)\n",
    "\n",
    "## <center> \\\\( f(x) = W^Tx+b \\\\)</center>\n",
    "\n",
    "### linear function을 완성하세요.\n",
    "### (Hint : (1) weight, bias 선언 (2) tf.get_variable()의 initializer  (3) tf.matmul())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc(name, out_dim, inputs):\n",
    "    \"\"\"\n",
    "    Args :\n",
    "        Inputs : Input tensor\n",
    "        out_dim : output dimension\n",
    "        \n",
    "    Returns:\n",
    "        inputs * weight + bias\n",
    "    \"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        ###################################################################\n",
    "        #                    Implementation 2                             #\n",
    "        ###################################################################\n",
    "        shp = inputs.get_shape().as_list()[-1]\n",
    "        init = tf.truncated_normal([shp, out_dim], stddev=.01)\n",
    "        weight = tf.get_variable('w', initializer=init)\n",
    "        \n",
    "        init = tf.constant(1.0, shape=[out_dim])\n",
    "        bias = tf.get_variable('b', initializer=init)\n",
    "        \n",
    "        result = tf.matmul(inputs, weight) + bias\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation (3)\n",
    "\n",
    "## Model Setting\n",
    "\n",
    "### 1. Training data 및 Test data의 각각의 image를 한 vector로 만들어서 train_data, test_data에 각각 저장하세요.\n",
    "#### Hint) 데이터 차원.\n",
    "### 2. Dataset로부터 받은 데이터(Image, label)를 담을 변수를 각각 x 및 y_true에 선언하세요.\n",
    "#### Hint) tf.placeholder\n",
    "### 3. Implementation (2)에서 구현한 linear classifier 함수값과 softmax 함수를 통한 prediction 값을 y_hat에 저장하세요.\n",
    "#### Hint) tf.nn.softmax\n",
    "### 4. 3으로부터 얻은 결과를 통해 Implementation (1)에서 구현한 loss function을 통해 얻은 loss를 cross_entropy에 저장하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th iteration, loss: 2.2496, test accuracy: 0.3584\n",
      "20th iteration, loss: 1.9549, test accuracy: 0.4820\n",
      "30th iteration, loss: 1.3699, test accuracy: 0.4656\n",
      "40th iteration, loss: 1.0314, test accuracy: 0.6863\n",
      "50th iteration, loss: 0.6739, test accuracy: 0.7034\n",
      "60th iteration, loss: 0.9111, test accuracy: 0.7647\n",
      "70th iteration, loss: 0.5880, test accuracy: 0.7875\n",
      "80th iteration, loss: 0.4280, test accuracy: 0.8200\n",
      "90th iteration, loss: 0.5563, test accuracy: 0.8532\n",
      "100th iteration, loss: 0.5077, test accuracy: 0.8555\n",
      "110th iteration, loss: 0.3837, test accuracy: 0.8736\n",
      "120th iteration, loss: 0.4810, test accuracy: 0.8757\n",
      "130th iteration, loss: 0.3460, test accuracy: 0.8846\n",
      "140th iteration, loss: 0.3631, test accuracy: 0.8801\n",
      "150th iteration, loss: 0.3722, test accuracy: 0.8880\n",
      "160th iteration, loss: 0.4169, test accuracy: 0.8772\n",
      "170th iteration, loss: 0.5444, test accuracy: 0.8866\n",
      "180th iteration, loss: 0.3670, test accuracy: 0.8934\n",
      "190th iteration, loss: 0.2883, test accuracy: 0.8978\n",
      "200th iteration, loss: 0.2113, test accuracy: 0.9020\n",
      "210th iteration, loss: 0.3277, test accuracy: 0.8935\n",
      "220th iteration, loss: 0.3826, test accuracy: 0.8994\n",
      "230th iteration, loss: 0.4694, test accuracy: 0.9071\n",
      "240th iteration, loss: 0.2887, test accuracy: 0.9001\n",
      "250th iteration, loss: 0.3467, test accuracy: 0.9077\n",
      "260th iteration, loss: 0.4305, test accuracy: 0.9094\n",
      "270th iteration, loss: 0.2798, test accuracy: 0.9066\n",
      "280th iteration, loss: 0.3066, test accuracy: 0.9087\n",
      "290th iteration, loss: 0.1347, test accuracy: 0.9125\n",
      "300th iteration, loss: 0.1699, test accuracy: 0.9124\n",
      "310th iteration, loss: 0.2740, test accuracy: 0.9163\n",
      "320th iteration, loss: 0.1410, test accuracy: 0.9158\n",
      "330th iteration, loss: 0.2184, test accuracy: 0.9176\n",
      "340th iteration, loss: 0.2444, test accuracy: 0.9158\n",
      "350th iteration, loss: 0.3434, test accuracy: 0.9205\n",
      "360th iteration, loss: 0.2641, test accuracy: 0.9246\n",
      "370th iteration, loss: 0.2933, test accuracy: 0.9204\n",
      "380th iteration, loss: 0.3548, test accuracy: 0.9117\n",
      "390th iteration, loss: 0.2655, test accuracy: 0.9226\n",
      "400th iteration, loss: 0.2727, test accuracy: 0.9265\n",
      "410th iteration, loss: 0.2134, test accuracy: 0.9236\n",
      "420th iteration, loss: 0.2369, test accuracy: 0.9253\n",
      "430th iteration, loss: 0.3302, test accuracy: 0.9204\n",
      "440th iteration, loss: 0.2773, test accuracy: 0.9298\n",
      "450th iteration, loss: 0.1876, test accuracy: 0.9271\n",
      "460th iteration, loss: 0.1873, test accuracy: 0.9337\n",
      "470th iteration, loss: 0.1838, test accuracy: 0.9319\n",
      "480th iteration, loss: 0.2863, test accuracy: 0.9338\n",
      "490th iteration, loss: 0.1590, test accuracy: 0.9294\n",
      "500th iteration, loss: 0.2887, test accuracy: 0.9339\n",
      "510th iteration, loss: 0.2141, test accuracy: 0.9326\n",
      "520th iteration, loss: 0.3141, test accuracy: 0.9372\n",
      "530th iteration, loss: 0.1381, test accuracy: 0.9363\n",
      "540th iteration, loss: 0.1964, test accuracy: 0.9351\n",
      "550th iteration, loss: 0.1156, test accuracy: 0.9370\n",
      "560th iteration, loss: 0.2767, test accuracy: 0.9304\n",
      "570th iteration, loss: 0.2260, test accuracy: 0.9369\n",
      "580th iteration, loss: 0.1661, test accuracy: 0.9419\n",
      "590th iteration, loss: 0.1313, test accuracy: 0.9387\n",
      "600th iteration, loss: 0.1369, test accuracy: 0.9359\n",
      "610th iteration, loss: 0.1342, test accuracy: 0.9391\n",
      "620th iteration, loss: 0.2942, test accuracy: 0.9440\n",
      "630th iteration, loss: 0.1732, test accuracy: 0.9404\n",
      "640th iteration, loss: 0.1337, test accuracy: 0.9437\n",
      "650th iteration, loss: 0.1278, test accuracy: 0.9468\n",
      "660th iteration, loss: 0.0932, test accuracy: 0.9450\n",
      "670th iteration, loss: 0.1528, test accuracy: 0.9453\n",
      "680th iteration, loss: 0.1545, test accuracy: 0.9438\n",
      "690th iteration, loss: 0.1284, test accuracy: 0.9475\n",
      "700th iteration, loss: 0.1419, test accuracy: 0.9465\n",
      "710th iteration, loss: 0.1590, test accuracy: 0.9483\n",
      "720th iteration, loss: 0.1569, test accuracy: 0.9418\n",
      "730th iteration, loss: 0.0633, test accuracy: 0.9478\n",
      "740th iteration, loss: 0.0739, test accuracy: 0.9462\n",
      "750th iteration, loss: 0.1264, test accuracy: 0.9465\n",
      "760th iteration, loss: 0.1739, test accuracy: 0.9455\n",
      "770th iteration, loss: 0.2084, test accuracy: 0.9497\n",
      "780th iteration, loss: 0.1317, test accuracy: 0.9522\n",
      "790th iteration, loss: 0.2250, test accuracy: 0.9491\n",
      "800th iteration, loss: 0.1291, test accuracy: 0.9492\n",
      "810th iteration, loss: 0.1640, test accuracy: 0.9508\n",
      "820th iteration, loss: 0.2272, test accuracy: 0.9511\n",
      "830th iteration, loss: 0.1342, test accuracy: 0.9553\n",
      "840th iteration, loss: 0.0765, test accuracy: 0.9535\n",
      "850th iteration, loss: 0.0757, test accuracy: 0.9527\n",
      "860th iteration, loss: 0.1548, test accuracy: 0.9496\n",
      "870th iteration, loss: 0.1637, test accuracy: 0.9536\n",
      "880th iteration, loss: 0.0959, test accuracy: 0.9563\n",
      "890th iteration, loss: 0.0456, test accuracy: 0.9506\n",
      "900th iteration, loss: 0.1954, test accuracy: 0.9550\n",
      "910th iteration, loss: 0.1158, test accuracy: 0.9537\n",
      "920th iteration, loss: 0.1788, test accuracy: 0.9511\n",
      "930th iteration, loss: 0.1259, test accuracy: 0.9578\n",
      "940th iteration, loss: 0.1180, test accuracy: 0.9575\n",
      "950th iteration, loss: 0.0907, test accuracy: 0.9584\n",
      "960th iteration, loss: 0.0642, test accuracy: 0.9580\n",
      "970th iteration, loss: 0.1886, test accuracy: 0.9569\n",
      "980th iteration, loss: 0.2048, test accuracy: 0.9592\n",
      "990th iteration, loss: 0.0652, test accuracy: 0.9588\n",
      "1000th iteration, loss: 0.2477, test accuracy: 0.9586\n",
      "1010th iteration, loss: 0.1238, test accuracy: 0.9558\n",
      "1020th iteration, loss: 0.1482, test accuracy: 0.9598\n",
      "1030th iteration, loss: 0.0584, test accuracy: 0.9579\n",
      "1040th iteration, loss: 0.1787, test accuracy: 0.9600\n",
      "1050th iteration, loss: 0.1077, test accuracy: 0.9597\n",
      "1060th iteration, loss: 0.0556, test accuracy: 0.9605\n",
      "1070th iteration, loss: 0.1286, test accuracy: 0.9607\n",
      "1080th iteration, loss: 0.1396, test accuracy: 0.9596\n",
      "1090th iteration, loss: 0.2150, test accuracy: 0.9597\n",
      "1100th iteration, loss: 0.1554, test accuracy: 0.9564\n",
      "1110th iteration, loss: 0.1051, test accuracy: 0.9608\n",
      "1120th iteration, loss: 0.0803, test accuracy: 0.9610\n",
      "1130th iteration, loss: 0.0369, test accuracy: 0.9630\n",
      "1140th iteration, loss: 0.1381, test accuracy: 0.9626\n",
      "1150th iteration, loss: 0.0872, test accuracy: 0.9608\n",
      "1160th iteration, loss: 0.0959, test accuracy: 0.9619\n",
      "1170th iteration, loss: 0.0986, test accuracy: 0.9604\n",
      "1180th iteration, loss: 0.1437, test accuracy: 0.9623\n",
      "1190th iteration, loss: 0.0815, test accuracy: 0.9607\n",
      "1200th iteration, loss: 0.0470, test accuracy: 0.9604\n",
      "1210th iteration, loss: 0.1657, test accuracy: 0.9624\n",
      "1220th iteration, loss: 0.0826, test accuracy: 0.9583\n",
      "1230th iteration, loss: 0.1105, test accuracy: 0.9629\n",
      "1240th iteration, loss: 0.0739, test accuracy: 0.9622\n",
      "1250th iteration, loss: 0.1722, test accuracy: 0.9625\n",
      "1260th iteration, loss: 0.0739, test accuracy: 0.9647\n",
      "1270th iteration, loss: 0.0697, test accuracy: 0.9637\n",
      "1280th iteration, loss: 0.0753, test accuracy: 0.9636\n",
      "1290th iteration, loss: 0.0521, test accuracy: 0.9628\n",
      "1300th iteration, loss: 0.0300, test accuracy: 0.9650\n",
      "1310th iteration, loss: 0.0711, test accuracy: 0.9649\n",
      "1320th iteration, loss: 0.0517, test accuracy: 0.9636\n",
      "1330th iteration, loss: 0.1006, test accuracy: 0.9644\n",
      "1340th iteration, loss: 0.1214, test accuracy: 0.9598\n",
      "1350th iteration, loss: 0.0672, test accuracy: 0.9647\n",
      "1360th iteration, loss: 0.1165, test accuracy: 0.9647\n",
      "1370th iteration, loss: 0.1017, test accuracy: 0.9665\n",
      "1380th iteration, loss: 0.1724, test accuracy: 0.9641\n",
      "1390th iteration, loss: 0.0709, test accuracy: 0.9616\n",
      "1400th iteration, loss: 0.0887, test accuracy: 0.9657\n",
      "1410th iteration, loss: 0.0337, test accuracy: 0.9654\n",
      "1420th iteration, loss: 0.0952, test accuracy: 0.9662\n",
      "1430th iteration, loss: 0.0377, test accuracy: 0.9690\n",
      "1440th iteration, loss: 0.1538, test accuracy: 0.9680\n",
      "1450th iteration, loss: 0.0490, test accuracy: 0.9662\n",
      "1460th iteration, loss: 0.0417, test accuracy: 0.9667\n",
      "1470th iteration, loss: 0.0656, test accuracy: 0.9671\n",
      "1480th iteration, loss: 0.0690, test accuracy: 0.9678\n",
      "1490th iteration, loss: 0.0488, test accuracy: 0.9649\n",
      "1500th iteration, loss: 0.0362, test accuracy: 0.9679\n",
      "1510th iteration, loss: 0.1355, test accuracy: 0.9635\n",
      "1520th iteration, loss: 0.0845, test accuracy: 0.9665\n",
      "1530th iteration, loss: 0.0649, test accuracy: 0.9671\n",
      "1540th iteration, loss: 0.0575, test accuracy: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550th iteration, loss: 0.0662, test accuracy: 0.9678\n",
      "1560th iteration, loss: 0.1168, test accuracy: 0.9689\n",
      "1570th iteration, loss: 0.0983, test accuracy: 0.9643\n",
      "1580th iteration, loss: 0.0875, test accuracy: 0.9653\n",
      "1590th iteration, loss: 0.1466, test accuracy: 0.9670\n",
      "1600th iteration, loss: 0.1029, test accuracy: 0.9668\n",
      "1610th iteration, loss: 0.0879, test accuracy: 0.9684\n",
      "1620th iteration, loss: 0.0445, test accuracy: 0.9680\n",
      "1630th iteration, loss: 0.1186, test accuracy: 0.9683\n",
      "1640th iteration, loss: 0.2052, test accuracy: 0.9680\n",
      "1650th iteration, loss: 0.1085, test accuracy: 0.9677\n",
      "1660th iteration, loss: 0.0519, test accuracy: 0.9679\n",
      "1670th iteration, loss: 0.1033, test accuracy: 0.9670\n",
      "1680th iteration, loss: 0.0802, test accuracy: 0.9690\n",
      "1690th iteration, loss: 0.0542, test accuracy: 0.9700\n",
      "1700th iteration, loss: 0.0983, test accuracy: 0.9674\n",
      "1710th iteration, loss: 0.0231, test accuracy: 0.9688\n",
      "1720th iteration, loss: 0.0361, test accuracy: 0.9678\n",
      "1730th iteration, loss: 0.0588, test accuracy: 0.9693\n",
      "1740th iteration, loss: 0.0486, test accuracy: 0.9698\n",
      "1750th iteration, loss: 0.1050, test accuracy: 0.9685\n",
      "1760th iteration, loss: 0.0408, test accuracy: 0.9695\n",
      "1770th iteration, loss: 0.0291, test accuracy: 0.9703\n",
      "1780th iteration, loss: 0.0201, test accuracy: 0.9677\n",
      "1790th iteration, loss: 0.0882, test accuracy: 0.9696\n",
      "1800th iteration, loss: 0.0669, test accuracy: 0.9721\n",
      "1810th iteration, loss: 0.0462, test accuracy: 0.9688\n",
      "1820th iteration, loss: 0.0895, test accuracy: 0.9710\n",
      "1830th iteration, loss: 0.0216, test accuracy: 0.9694\n",
      "1840th iteration, loss: 0.1553, test accuracy: 0.9681\n",
      "1850th iteration, loss: 0.1402, test accuracy: 0.9731\n",
      "1860th iteration, loss: 0.0861, test accuracy: 0.9670\n",
      "1870th iteration, loss: 0.0642, test accuracy: 0.9671\n",
      "1880th iteration, loss: 0.1043, test accuracy: 0.9720\n",
      "1890th iteration, loss: 0.0554, test accuracy: 0.9710\n",
      "1900th iteration, loss: 0.0174, test accuracy: 0.9717\n",
      "1910th iteration, loss: 0.0420, test accuracy: 0.9704\n",
      "1920th iteration, loss: 0.0284, test accuracy: 0.9701\n",
      "1930th iteration, loss: 0.2045, test accuracy: 0.9723\n",
      "1940th iteration, loss: 0.0584, test accuracy: 0.9704\n",
      "1950th iteration, loss: 0.0497, test accuracy: 0.9705\n",
      "1960th iteration, loss: 0.0588, test accuracy: 0.9704\n",
      "1970th iteration, loss: 0.0357, test accuracy: 0.9697\n",
      "1980th iteration, loss: 0.1229, test accuracy: 0.9701\n",
      "1990th iteration, loss: 0.0337, test accuracy: 0.9687\n",
      "2000th iteration, loss: 0.0588, test accuracy: 0.9711\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "# Flatten data\n",
    "###################################################################\n",
    "#                    Implementation 3-1                           #\n",
    "###################################################################\n",
    "# train_data = None\n",
    "# test_data = None\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-2                           #\n",
    "###################################################################\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784]) # 배치사이즈로 넣어줄 거라서 앞의 shape 는 None 이 된다.\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-3                          #\n",
    "###################################################################\n",
    "h1 = fc('layer1', 512, x)\n",
    "h1 = tf.nn.relu(h1)\n",
    "h2 = fc('layer2', 64, h1)\n",
    "h2 = tf.nn.relu(h2)\n",
    "y_logits = fc('layer3', 10, h2)\n",
    "\n",
    "\n",
    "y_hat = tf.nn.softmax(y_logits)\n",
    "\n",
    "###################################################################\n",
    "#                    Implementation 3-4                           #\n",
    "###################################################################\n",
    "cross_entropy = get_cross_entropy_loss(y_true, y_hat)\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = get_accuracy(y_true, y_hat)\n",
    "# Make gradient descent op\n",
    "train_step = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# Make op to initialize declared variable\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize variables\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training\n",
    "    for step in range(max_iter):\n",
    "        # Get batch data and label\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # train the network and calculate cross entropy\n",
    "        _= sess.run(train_step, feed_dict={x: batch_x, y_true: batch_y}) # w, b 를 update하자\n",
    "        loss = sess.run(cross_entropy, feed_dict={x: batch_x, y_true: batch_y}) # loss를 확인하자\n",
    "        \n",
    "        # calcualte accuracy\n",
    "        acc = sess.run(accuracy, feed_dict={x: test_data, y_true: test_label})\n",
    "        # print loss (cross entropy) and accuracy at every 10th step\n",
    "        if (step + 1) % 10 == 0:\n",
    "            print(\"{}th iteration, loss: {:.4f}, test accuracy: {:.4f}\".format(step + 1, loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
